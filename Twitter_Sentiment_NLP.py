# -*- coding: utf-8 -*-
"""Proyek_NLP3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YxXy5ddVH6m6GENcAFgJOx0rBOCPs4Y3
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import nltk
import os
import re
import string
from keras.layers import LSTM, Dropout, Dense, Embedding
from keras.models import Model
from keras.callbacks import Callback, ModelCheckpoint
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical, plot_model
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')


from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis

!unzip twitter-entity-sentiment-analysis.zip

df = pd.read_csv('/content/twitter_training.csv', nrows = 3000)
df.head(5)

df.rename(columns={'Positive': 'sentiment',
                   'im getting on borderlands and i will murder you all ,': 'tweet_content'},inplace=True)

df_sentiment = df.drop(columns=['2401', 'Borderlands'])
df_sentiment.head(5)

df_sentiment["sentiment"].hist()

df_sentiment["tweet_content"] = df_sentiment["tweet_content"].astype(str)
df_sentiment["sentiment"] = df_sentiment["sentiment"].astype(str)

df_sentiment.info()

df_sentiment.isnull().sum()

df_sentiment.columns

df_sentiment.shape

df_sentiment.sentiment.value_counts()

df_sentiment.tweet_content = df_sentiment.tweet_content.apply(lambda x: x.lower())

def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    df_sentiment.tweet_content = df_sentiment.tweet_content.apply(lambda x: cleaner(x))

lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    df_sentiment.tweet_content = df_sentiment.tweet_content.apply(lambda x: lem(x))

def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    df_sentiment['tweet_content'].apply(rem_numbers)

st_words = stopwords.words()
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    df_sentiment.tweet_content = df_sentiment.tweet_content.apply(lambda x: stopword(x))

df_sentiment.head(5)

sentiment = pd.get_dummies(df_sentiment.sentiment)
df_sentiment_new = pd.concat([df_sentiment, sentiment], axis=1)
df_sentiment_new = df_sentiment_new.drop(columns='sentiment')
df_sentiment_new.head(5)

tweet = df_sentiment_new['tweet_content'].values
labels = df_sentiment_new[['Irrelevant', 'Negative', 'Neutral', 'Positive']].values

labels

tweet_train, tweet_test, labels_train, labels_test = train_test_split(tweet, labels, test_size=0.2, shuffle=True)

print("tweet_train:", len(tweet_train))
print("tweet_test:", len(tweet_test))
print("labels_train:", len(labels_train))
print("labels_test:", len(labels_test))

vocab_size = 5000
oov_tok = 'x'
symbols = '!"#$%&()*+,-./:;<=>@[\]^_`{|}~ '

tokenizer = Tokenizer(num_words = vocab_size,
                      oov_token = oov_tok,
                      filters = symbols)

tokenizer.fit_on_texts(tweet_train)

sekuens_train = tokenizer.texts_to_sequences(tweet_train)
sekuens_test = tokenizer.texts_to_sequences(tweet_test)

padded_train = pad_sequences(sekuens_train)
padded_test = pad_sequences(sekuens_test)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(4, activation='softmax')
])


model.compile(optimizer='adam',
              metrics=['accuracy'],
              loss='categorical_crossentropy',)

model.summary()

plot_model(model)

accuracy_threshold = 0.9
class stop(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if logs.get('accuracy') > accuracy_threshold and logs.get('val_accuracy') > accuracy_threshold:
      print('\nFor Epoch', epoch, '\nAccuracy has reach = %2.2f%%' %(logs['accuracy']*100), 'training has been stopped.')
      self.model.stop_training=True
callbacks=stop()

checkpoint_path = "model.h5"
checkpoint = ModelCheckpoint(checkpoint_path,
                             save_best_only=True
                             )

num_epoch = 200
history = model.fit(padded_train,
                    labels_train,
                    epochs=num_epoch,
                    validation_data=(padded_test, labels_test),
                    verbose=2,
                    callbacks=[callbacks, checkpoint],
                    validation_steps=30)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()